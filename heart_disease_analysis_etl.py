# -*- coding: utf-8 -*-
"""heart_disease_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vjdRWTArrS3vJykDWf57qe0-5HiMbM76

# Config. do Colab
"""

!pip install gdown
import gdown
import pandas as pd
import pandas_gbq
from google.oauth2 import service_account

"""# ETL

## Extract

<h2>Armazenamento de links para download de dataset desejado</h2>
"""

# Baixados do site: https://archive.ics.uci.edu/dataset/45/heart+disease e armazenados numa pasta no Google Drive de um dos integrantes do grupo

stored_data_urls = {'raw_data.csv':'https://drive.google.com/file/d/1Z6tJsVQVb6FSRY1HBnNSjMECmrvT1kit/view?usp=drive_link',
                    'data.csv':'https://drive.google.com/file/d/1aSdmbsFGgr80cjF2frrqAopbxaoGcoXu/view?usp=sharing',
                    'heart.csv':'https://drive.google.com/file/d/1xLjDaRmRpRMhUBm5iYokj8L0Q-Ed2C9i/view?usp=sharing'}

"""<h2>Baixando dataset neste ambiente</h2>"""

dataset = 'raw_data.csv'

url = stored_data_urls[dataset]

output = dataset

output_filepath = gdown.download(url=url, output=output, fuzzy=True,quiet=True)

"""<h3>Gerando um DataFrame Pandas para realizar análises e manipulações</h3>"""

df = pd.read_csv(output_filepath)

df.head()

df.describe(include='all')

"""## Transform

<h3>Contando quantos valores estão vazios</h3>
"""

count = 0
for i in df.columns:
    for j in df[i]:
        if j=="?":
            count += 1
print("Valores faltantes: ", count)

"""<h3>Filtrando o dataframe para apenas linhas que não contenham valores iguais a "?"</h3>"""

cleaned_df = df[~(df == "?").any(axis=1)]
cleaned_df

cleaned_data = cleaned_df.astype(float)

"""<h3>Mostrando dados limpos</h3>"""

cleaned_data.head()

cleaned_data.describe(include='all')

cleaned_data.to_csv('dados_ml.csv',index=False)
cleaned_data

cleaned_df = cleaned_df.copy()
cleaned_df['sex'] = ['Masculino' if value == 1 else 'Feminino' for value in cleaned_df['sex']]

cleaned_df

dados = cleaned_df.copy()

"""<h3>Adicionando um ID para cada paciente no dataset</h3>"""

import uuid

def adicionar_coluna_ID(nome_coluna_id : str, dataframe : pd.DataFrame, coluna_fixa : str = 'Paciente_ID') -> pd.DataFrame:
  dataframe = dataframe.copy()
  dataframe[nome_coluna_id] = [str(uuid.uuid4()) for _ in dataframe[coluna_fixa]]
  dataframe = dataframe[[nome_coluna_id]+[col for col in dataframe.columns if col != nome_coluna_id]]
  return dataframe

dados = dados.copy()

dados['Paciente_ID'] = [str(uuid.uuid4()) for _ in dados['age']]

dados = dados[['Paciente_ID'] + [col for col in dados.columns if col != 'Paciente_ID']]

dados

# Convertendo apenas 'col1' e 'col2' para float
cols_to_convert = [column for column in dados.columns if column not in ['Paciente_ID','sex']]
dados[cols_to_convert] = dados[cols_to_convert].astype(float)

print(dados.dtypes)

"""Renomenado a coluna "num" para "default_target" (que varia de 0 a 4, com 0 sem doença e 1 a 4 doente com grau de severidade) e criando uma nova coluna "target" (que varia de 0 a 1, com 0 sem doença e 1 com doença cardíaca)."""

# Renomenado coluna de target
dados.rename(columns={'num':'default_target'},inplace=True)

# Criando coluna de target binária
dados['target'] = [0 if value == 0 else 1 for value in dados['default_target']]

"""<h3>Criando dimensão Paciente</h3>"""

dim_paciente = dados[['Paciente_ID','age','sex']]

dim_paciente

"""<h3>Criando dimensão Exame_Esforco</h3>"""

dim_exames_esforco = dados[['Paciente_ID','restecg','oldpeak','slope','exang','thalach','trestbps']]

dim_exames_esforco = adicionar_coluna_ID(nome_coluna_id='Exame_Esforco_ID',dataframe=dim_exames_esforco)

dim_exames_esforco

"""<h3>Criando dimensão Exame_Coleta</h3>"""

dim_exames_coleta = dados[['Paciente_ID','thal','chol','ca','fbs']]

dim_exames_coleta = adicionar_coluna_ID(nome_coluna_id='Exame_Coleta_ID',dataframe=dim_exames_coleta)

dim_exames_coleta

"""<h3>Criando dimensão Diagnostico</h3>"""

dim_diagnostico = dados[['Paciente_ID','default_target','target']]
dim_diagnostico = adicionar_coluna_ID(nome_coluna_id='Diagnostico_ID',dataframe=dim_diagnostico)

dim_diagnostico

"""<h3>Criando dimensão Sintoma</h3>"""

dim_sintoma = dados[['Paciente_ID','cp']]
dim_sintoma = adicionar_coluna_ID(nome_coluna_id='Sintoma_ID',dataframe=dim_sintoma)

dim_sintoma

"""<h3>Criando tabela Fato_Exames</h3>

Esta será para criação dos indicadores
"""

# Merge inicial: Paciente com Exames_Esforço
merge_paciente_esforco = pd.merge(
    dim_paciente,
    dim_exames_esforco,
    on='Paciente_ID',
    how='inner'
)

# Merge final: Adicionar Exames_Coleta
fato_exames_analises = pd.merge(
    merge_paciente_esforco,
    dim_exames_coleta,
    on='Paciente_ID',
    how='inner'
)

colunas_ordenadas = ['Paciente_ID','Exame_Esforco_ID','Exame_Coleta_ID']
fato_exames_analises = fato_exames_analises[colunas_ordenadas+[col for col in fato_exames_analises.columns if col not in colunas_ordenadas]]

fato_exames_analises.to_csv('fato_exames_para_gerar_analises.csv',index=False)
fato_exames_analises

def analisar_restecg_indicador(restecg:float) -> str:
  if restecg == 0:
    return 'normal'
  elif restecg == 1:
    return 'ter anomalia da onda ST-T'
  elif restecg == 2:
    return 'hipertrofia ventricular esquerda'

def analisar_chol_indicador(chol:float) -> str:
  if chol <= 200:
    return 'baixo'
  elif chol >=200 and chol <= 239:
    return 'moderado'
  elif chol >= 240:
    return 'alto'

def analisar_thalach_age_indicador(thalach:float,age:float) -> str:
  """
  Referência: https://veja.abril.com.br/saude/qual-e-a-frequencia-cardiaca-ideal-durante-o-exercicio#:~:text=De%20acordo%20com%20a%20Associa%C3%A7%C3%A3o,100%20bpm%20e%20170%20bpm
  """
  if age in range(18,25): # Entre 18 e 25 anos (números pra 20 anos)
    if thalach >= 100 and thalach <= 170:
      return 'adequado'
    else:
      return 'inadequado'
  elif age in range(25,33): # Entre 25 e 32 anos (números pra 30 anos)
    if thalach >= 95 and thalach <= 162:
      return 'adequado'
    else:
      return 'inadequado'
  elif age in range(33,38): # Entre 33 e 37 anos (números pra 35 anos)
    if thalach >= 93 and thalach <= 157:
      return 'adequado'
    else:
      return 'inadequado'
  elif age in range(38,43): # Entre 38 e 42 anos (números pra 40 anos)
    if thalach >= 90 and thalach <= 153:
      return 'adequado'
    else:
      return 'inadequado'
  elif age in range(43,48): # Entre 43 e 47 anos (números pra 45 anos)
    if thalach >= 88 and thalach <= 149:
      return 'adequado'
    else:
      return 'inadequado'
  elif age in range(48,53): # Entre 48 e 52 anos (números pra 50 anos)
    if thalach >= 85 and thalach <= 145:
      return 'adequado'
    else:
      return 'inadequado'
  elif age in range(53,58): # Entre 53 e 57 anos (números pra 55 anos)
    if thalach >= 83 and thalach <= 140:
      return 'adequado'
    else:
      return 'inadequado'
  elif age in range(58,63): # Entre 58 e 62 anos (números pra 60 anos)
    if thalach >= 80 and thalach <= 136:
      return 'adequado'
    else:
      return 'inadequado'
  elif age in range(63,68): # Entre 63 e 67 anos (números pra 65 anos)
    if thalach >= 78 and thalach <= 132:
      return 'adequado'
    else:
      return 'inadequado'
  elif age in range(68,80): # Entre 68 e 80 anos (números pra 70 anos)
    if thalach >= 75 and thalach <= 128:
      return 'adequado'
    else:
      return 'inadequado'
  else:
    return '?'

def analisar_trestbps_age_indicador(trestbps:float,age:float) -> str:
  """
  Referência para bpm (não utilizado): https://www.hospitalimigrantes.com.br/post/qual-a-frequencia-cardiaca-normal-por-idade-e-como-avaliar/24/#:~:text=De%208%20at%C3%A9%2017%20anos,anos%2050%20a%206%20bpm
  Referência para mmHg (utilizado): https://telemedicinamorsch.com.br/blog/tabela-de-pressao-arterial
  """
  # # em bpm
  # if age < 18:
  #   if trestbps >=80 and trestbps <= 100:
  #     return 'adequado'
  #   else:
  #     return 'inadequado'
  # elif age >= 18 and age <= 65:
  #   if trestbps >=70 and trestbps <= 78:
  #     return 'adequado'
  #   else:
  #     return 'inadequado'
  # elif age > 65:
  #   if trestbps >=50 and trestbps <= 60:
  #     return 'adequado'
  #   else:
  #     return 'inadequado'
  # else:
  #   return '?'

  # em mmHg
  if age >= 19 and age <= 29:
    if trestbps < 121:
      return 'adequado'
    else:
      return 'inadequado'
  elif age >= 30 and age <= 35:
    if trestbps < 123:
      return 'adequado'
    else:
      return 'inadequado'
  elif age >= 36 and age <= 39:
    if trestbps < 124:
      return 'adequado'
    else:
      return 'inadequado'
  elif age >= 40 and age <= 45:
    if trestbps < 125:
      return 'adequado'
    else:
      return 'inadequado'
  elif age >= 46 and age <= 49:
    if trestbps < 127:
      return 'adequado'
    else:
      return 'inadequado'
  elif age >= 50 and age <= 55:
    if trestbps < 128:
      return 'adequado'
    else:
      return 'inadequado'
  elif age >= 55 and age <= 59:
    if trestbps < 131:
      return 'adequado'
    else:
      return 'inadequado'
  elif age >= 60:
    if trestbps < 135:
      return 'adequado'
    else:
      return 'inadequado'
  return '?'

def analisar_oldpeak_ajustado_indicador(slope:float,oldpeak:float) -> str:
  if slope == 1:
    if oldpeak <= 1:
      return 'bom'
    else:
      return 'ruim'
  elif slope == 2:
    if oldpeak <= 2:
      return 'moderado'
    elif oldpeak > 2:
      return 'ruim'
  elif slope == 3:
    return 'ruim'
  else:
    return '?'

def analisar_exang_indicador(exang:float) -> str:
  if exang == 1:
    return 'sim'
  elif exang == 0:
    return 'não'
  else:
    return '?'

def analisar_fbs_indicador(fbs:float) -> str:
  if fbs == 0:
    return 'adequado'
  return 'inadequado'


def analisar_thal_indicador(thal:float) -> str:
  if thal == 3:
    return 'normal'
  elif thal == 6:
    return 'defeito fixo'
  elif thal == 7:
    return 'defeito reversível'
  return '?'

"""<h3>Criando tabela Fato_Exames</h3>

Esta será para exibição dos indicadores
"""

colunas_fato_exames = ['Paciente_ID','Exame_Esforco_ID','Exame_Coleta_ID',
                       'restecg_indicador','chol_indicador','thalach_age_indicador',
                       'trestbps_age_indicador','oldpeak_ajustado_indicador','exang_indicador',
                       'fbs_indicador','thal_indicador']

fato_exames_analises = fato_exames_analises.copy()

fato_exames_analises['restecg_indicador'] = [analisar_restecg_indicador(restecg) for restecg in fato_exames_analises['restecg']]
fato_exames_analises['chol_indicador'] = [analisar_chol_indicador(chol) for chol in fato_exames_analises['chol']]
fato_exames_analises['thalach_age_indicador'] = [analisar_thalach_age_indicador(thalach,age) for thalach,age in zip(fato_exames_analises['thalach'],fato_exames_analises['age'])]
fato_exames_analises['trestbps_age_indicador'] = [analisar_trestbps_age_indicador(trestbps,age) for trestbps,age in zip(fato_exames_analises['trestbps'],fato_exames_analises['age'])]
fato_exames_analises['oldpeak_ajustado_indicador'] = [analisar_oldpeak_ajustado_indicador(slope,oldpeak) for slope,oldpeak in zip(fato_exames_analises['slope'],fato_exames_analises['oldpeak'])]
fato_exames_analises['exang_indicador'] = [analisar_exang_indicador(exang) for exang in fato_exames_analises['exang']]
fato_exames_analises['fbs_indicador'] = [analisar_fbs_indicador(fbs) for fbs in fato_exames_analises['fbs']]
fato_exames_analises['thal_indicador'] = [analisar_thal_indicador(thal) for thal in fato_exames_analises['thal']]

columns_to_drop = [column for column in fato_exames_analises.columns if column not in colunas_fato_exames]

fato_exame = fato_exames_analises.drop(columns=columns_to_drop,axis=1)

fato_exame.to_csv('fato_exame.csv',index=False)
fato_exame

"""<h3>Criando tabela Fato_Sintoma</h3>

Esta será para criação dos indicadores
"""

# Merge inicial: Paciente com Exames_Esforço
merge_paciente_diagnostico = pd.merge(
    dim_paciente,
    dim_diagnostico,
    on='Paciente_ID',
    how='inner'
)

# Merge final: Adicionar Exames_Coleta
fato_sintoma_analises = pd.merge(
    merge_paciente_diagnostico,
    dim_sintoma,
    on='Paciente_ID',
    how='inner'
)

colunas_ordenadas = ['Paciente_ID','Diagnostico_ID','Sintoma_ID']
fato_sintoma_analises = fato_sintoma_analises[colunas_ordenadas+[col for col in fato_sintoma_analises.columns if col not in colunas_ordenadas]]

fato_sintoma_analises.to_csv('fato_sintoma_para_gerar_analises.csv',index=False)
fato_sintoma_analises

def analisar_num_indicador(num:float) -> str:
  if num == 0:
    return 'Saudável'
  if num in range(1,5):
    return 'Doente'
  return '?'

def analisar_cp_indicador(cp:float) -> str:
  if cp == 1:
    return 'angina típica'
  elif cp == 2:
    return 'angina atípica'
  elif cp == 3:
    return 'dor não anginosa'
  elif cp == 4:
    return 'assintomático'
  return '?'

"""<h3>Criando tabela Fato_Sintoma</h3>

Esta será para exibição dos indicadores
"""

colunas_fato_diagnostico = ['Paciente_ID','Diagnostico_ID','Sintoma_ID',
                            'num_indicador','cp_indicador']

fato_sintoma_analises = fato_sintoma_analises.copy()

fato_sintoma_analises['target_indicador'] = [analisar_num_indicador(num) for num in fato_sintoma_analises['default_target']]
fato_sintoma_analises['cp_indicador'] = [analisar_cp_indicador(cp) for cp in fato_sintoma_analises['cp']]

columns_to_drop = [column for column in fato_sintoma_analises.columns if column not in colunas_fato_diagnostico]

fato_sintoma = fato_sintoma_analises.drop(columns=columns_to_drop,axis=1)

fato_sintoma.to_csv('fato_sintoma.csv',index=False)
fato_sintoma

"""Refatorando Dimensões"""

dim_paciente.to_csv('dim_paciente.csv',index=False)

dim_exames_esforco = dim_exames_esforco[[col for col in dim_exames_esforco.columns if col != 'Paciente_ID']]
dim_exames_esforco.to_csv('dim_exames_esforco.csv',index=False)

dim_exames_coleta = dim_exames_coleta[[col for col in dim_exames_coleta.columns if col != 'Paciente_ID']]
dim_exames_coleta.to_csv('dim_exames_coleta.csv',index=False)

dim_diagnostico = dim_diagnostico[[col for col in dim_diagnostico.columns if col != 'Paciente_ID']]
dim_diagnostico.to_csv('dim_diagnostico.csv',index=False)

dim_sintoma = dim_sintoma[[col for col in dim_sintoma.columns if col != 'Paciente_ID']]
dim_sintoma.to_csv('dim_sintoma.csv',index=False)

"""## Load (BigQuery)"""

# from google.colab import userdata
# private_key = userdata.get('PRIVATE_KEY')
# private_key_id = userdata.get('PRIVATE_KEY_ID')

# pandas_gbq.context.credentials = service_account.Credentials.from_service_account_info(
#     {
#   "type": "service_account",
#   "project_id": "projeto-data-warehouse-ufsc",
#   "private_key_id": private_key_id,
#   "private_key": private_key,
#   "client_email": "colab-272@projeto-data-warehouse-ufsc.iam.gserviceaccount.com",
#   "client_id": "115052981519937082715",
#   "auth_uri": "https://accounts.google.com/o/oauth2/auth",
#   "token_uri": "https://oauth2.googleapis.com/token",
#   "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
#   "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/colab-272%40projeto-data-warehouse-ufsc.iam.gserviceaccount.com",
#   "universe_domain": "googleapis.com"
# }
# )

# pandas_gbq.context.project = 'projeto-data-warehouse-ufsc'

# pandas_gbq.to_gbq(cleaned_data,'dataset.data',project_id='projeto-data-warehouse-ufsc',if_exists='replace')

# pandas_gbq.to_gbq(dim_paciente,'warehouse.dim_paciente',project_id='projeto-data-warehouse-ufsc',if_exists='replace')
# pandas_gbq.to_gbq(dim_exames_esforco,'warehouse.dim_exames_esforco',project_id='projeto-data-warehouse-ufsc',if_exists='replace')
# pandas_gbq.to_gbq(dim_exames_coleta,'warehouse.dim_exames_coleta',project_id='projeto-data-warehouse-ufsc',if_exists='replace')
# pandas_gbq.to_gbq(dim_diagnostico,'warehouse.dim_diagnostico',project_id='projeto-data-warehouse-ufsc',if_exists='replace')
# pandas_gbq.to_gbq(dim_sintoma,'warehouse.dim_sintoma',project_id='projeto-data-warehouse-ufsc',if_exists='replace')
# pandas_gbq.to_gbq(fato_exame,'warehouse.fato_exame',project_id='projeto-data-warehouse-ufsc',if_exists='replace')
# pandas_gbq.to_gbq(fato_sintoma,'warehouse.fato_sintoma',project_id='projeto-data-warehouse-ufsc',if_exists='replace')

"""# Machine Learning com dataset limpo"""

#IMPORTAR BIBLIOTECA NECESSARIAS
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()

from scipy import stats
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split, KFold, cross_val_score

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import roc_curve, auc
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import ExtraTreesClassifier

#LER O CSV
df_ml = pd.read_csv('dados_ml.csv', sep=",")
df_ml['target'] = [0 if value == 0 else 1 for value in df_ml['num']]
df_ml.drop(columns=['num'],axis=1, inplace=True)
df_ml.head(10)

plt.figure(figsize=(14, 14))
sns.heatmap(df_ml.corr(),annot=True,fmt='.1f')
plt.show()
#O método corr() do pandas calcula os coeficientes de correlação entre todas as variáveis numéricas do DataFrame
#Assim podemos garantir que todo dado é único e nem sempre tem uma forte relação com o resultado do tager
#O resultado do heatmap tem o seguinte padrão:
#1: Correlação positiva perfeita (quando uma variável aumenta, a outra também aumenta proporcionalmente).
#0: Nenhuma correlação linear (as variáveis não estão linearmente relacionadas).
#-1: Correlação negativa perfeita (quando uma variável aumenta, a outra diminui proporcionalmente).
#Variáveis como ch (chest pain), thalch, e slope têm alta correlação com a variável target (presença de doença cardíaca)

sns.countplot(x="target", hue="target", data=df_ml, palette=["#1f77b4", "#ff7f0e"], legend=False)
plt.show()
df_ml.info()
#Verificando que há uma boa distribuição de dados no dataset e que os dados estão limpos

df_ml = pd.get_dummies(df_ml, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])
df_ml.shape
#A função pd.get_dummies()
#do pandas realiza a transformação conhecida como one-hot encoding.
#Ela converte colunas categóricas em várias colunas binárias (0 ou 1)
#onde cada nova coluna representa uma categoria única da variável original.
standardScaler = StandardScaler()
columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
df_ml[columns_to_scale] = standardScaler.fit_transform(df_ml[columns_to_scale])
#Usando o standard scaler pra normalizar os dados e melhorar a convergencia

x = df_ml.drop(['target'], axis=1)
y = df_ml['target']# Dividindo os dados em X (features) e y (target)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Aplicando K-Fold Cross-Validation para diferentes valores de max_features
dt_scores = []
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for i in range(1, len(x.columns) + 1):
    dt_classifier = DecisionTreeClassifier(max_features=i, random_state=42)
    scores = cross_val_score(dt_classifier, x_train, y_train, cv=kf, scoring='accuracy')
    dt_scores.append([np.mean(scores), i])


x_ax = [i for i in range(1, len(x.columns) + 1)]
plt.plot(x_ax, [score[0] for score in dt_scores])

plt.xticks([i for i in range(1, len(x.columns) + 1)])
plt.xlabel('Max de Atributos')
plt.ylabel('Acurácia da Validação Cruzada')

plt.title('Decision Tree Classifier: Scores de Cross-Validation para diferentes max_features')
plt.show()


best_max_features = max(dt_scores, key=lambda x: x[0])[1]


dt = DecisionTreeClassifier(max_features=best_max_features, random_state=42)
dt.fit(x_train, y_train)

print(f"Training accuracy: {format(dt.score(x_train, y_train), '.4f')}")
predicted_dt = dt.predict(x_test)
print(f"Testing accuracy: {format(metrics.accuracy_score(y_test, predicted_dt), '.4f')}")

from sklearn.model_selection import learning_curve

# Gerar a curva de aprendizado
train_sizes, train_scores, valid_scores = learning_curve(
    DecisionTreeClassifier(max_features=best_max_features, random_state=42),
    x_train, y_train,
    cv=kf,
    scoring='accuracy',
    train_sizes=np.linspace(0.1, 1.0, 10),
    random_state=42
)

# Calcular a média e o desvio padrão das métricas
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
valid_mean = np.mean(valid_scores, axis=1)
valid_std = np.std(valid_scores, axis=1)

# Plotar a curva de aprendizado
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, 'o-', color="blue", label="Acurácia Treinamento")
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color="blue")

plt.plot(train_sizes, valid_mean, 'o-', color="green", label="Acurácia Validação")
plt.fill_between(train_sizes, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color="green")

plt.title("Curva de Aprendizado para Decision Tree")
plt.xlabel("Tamanho do Treinamento")
plt.ylabel("Acurácia")
plt.legend(loc="best")
plt.grid()
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, predicted_dt)

disp = ConfusionMatrixDisplay(confusion_matrix=cm)
fig, ax = plt.subplots(figsize=(8, 6))
disp.plot(cmap='Oranges', ax=ax)


TN, FP, FN, TP = cm.ravel()


ax.text(0, 0, f'TN: {TN}', ha='right', va='top', color='white', fontsize=14, fontweight='bold')
ax.text(1, 0, f'FN: {FN}', ha='right', va='top', color='black', fontsize=14, fontweight='bold')
ax.text(0, 1, f'FP: {FP}', ha='right', va='top', color='black', fontsize=14, fontweight='bold')
ax.text(1, 1, f'TP: {TP}', ha='right', va='top', color='white', fontsize=14, fontweight='bold')


plt.title('Matriz de Confusão para o Decision Tree Classifier')
plt.show()

# Visualizando a importância das features
importances = dt.feature_importances_

# Criando um gráfico de barras para exibir a importância das features
feature_importance_df = pd.DataFrame({
    'Atributo': x.columns,
    'Importância': importances
}).sort_values(by='Importância', ascending=False)

# Plotando a importância das features
plt.figure(figsize=(10, 6))
sns.barplot(x='Importância', y='Atributo', data=feature_importance_df)
plt.title('Importância dos Atributos para o Decision Tree Classifier')
plt.show()